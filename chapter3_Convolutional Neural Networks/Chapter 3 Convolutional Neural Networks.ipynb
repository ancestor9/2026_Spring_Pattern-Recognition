{"cells":[{"cell_type":"markdown","metadata":{"id":"v0bzlwY1qQkM"},"source":["# Chapter 3: Convolutional Neural Networks\n","You'll need to either copy the image training files you downloaded in Chapter 2 to this directory, or alter the paths appropriately."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mvts9vO5qQkT","executionInfo":{"status":"ok","timestamp":1769774473026,"user_tz":-540,"elapsed":20527,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from PIL import Image"]},{"cell_type":"markdown","metadata":{"id":"fKuzTIRnqQkZ"},"source":["## CNNNet (or AlexNet by another nameâ€¦)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"cX4h8KFRqQkb","executionInfo":{"status":"ok","timestamp":1769774473033,"user_tz":-540,"elapsed":3,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["class CNNNet(nn.Module):\n","\n","    def __init__(self, num_classes=2):\n","        super(CNNNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FnCue2iNqQkd","executionInfo":{"status":"ok","timestamp":1769774475094,"user_tz":-540,"elapsed":601,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["cnnnet = CNNNet()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Rv_jjKDMqQke","executionInfo":{"status":"ok","timestamp":1769774480367,"user_tz":-540,"elapsed":31,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n","    for epoch in range(1, epochs+1):\n","        training_loss = 0.0\n","        valid_loss = 0.0\n","        model.train()\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            inputs, targets = batch\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","            output = model(inputs)\n","            loss = loss_fn(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","            training_loss += loss.data.item() * inputs.size(0)\n","        training_loss /= len(train_loader.dataset)\n","\n","        model.eval()\n","        num_correct = 0\n","        num_examples = 0\n","        for batch in val_loader:\n","            inputs, targets = batch\n","            inputs = inputs.to(device)\n","            output = model(inputs)\n","            targets = targets.to(device)\n","            loss = loss_fn(output,targets)\n","            valid_loss += loss.data.item() * inputs.size(0)\n","            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1],\n","                               targets)\n","            num_correct += torch.sum(correct).item()\n","            num_examples += correct.shape[0]\n","        valid_loss /= len(val_loader.dataset)\n","\n","        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n","        valid_loss, num_correct / num_examples))"]},{"cell_type":"markdown","source":[],"metadata":{"id":"M_Vd1e-EysyD"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAFrak7XytKg","executionInfo":{"status":"ok","timestamp":1769774721561,"user_tz":-540,"elapsed":18991,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"402863aa-b42b-40b3-983a-08ee1ef1e719"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import zipfile\n","import shutil\n","import random\n","\n","# 1. í™˜ê²½ ì„¤ì •\n","base_path = '/content'\n","categories = ['cat', 'fish']\n","splits = ['train', 'val', 'test']\n","zip_path ='/content/drive/MyDrive/Deep Learning/beginners-pytorch-deep-learning-master/chapter2_Image Classification with PyTorch/images.zip'\n","temp_extract_path = '/content/temp_images'\n","\n","# 2. ìµœì¢… ëª©ì ì§€ ë””ë ‰í† ë¦¬ ìƒì„±\n","for s in splits:\n","    for c in categories:\n","        os.makedirs(os.path.join(base_path, s, c), exist_ok=True)\n","\n","# 3. ì••ì¶• í•´ì œ\n","zip_file_extracted = False\n","if os.path.exists(zip_path):\n","    # ê¸°ì¡´ ì„ì‹œ í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒì„±\n","    if os.path.exists(temp_extract_path):\n","        shutil.rmtree(temp_extract_path)\n","\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(temp_extract_path)\n","    print(\"âœ… 1. ì••ì¶• í•´ì œ ì™„ë£Œ\")\n","    zip_file_extracted = True\n","else:\n","    print(f\"âŒ Error: {zip_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","# 4. ì‹¤ì œ cat, fish í´ë”ì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜\n","def find_category_path(root_path, target_category):\n","    for root, dirs, files in os.walk(root_path):\n","        if target_category in dirs:\n","            return os.path.join(root, target_category)\n","    return None\n","\n","# 5. í´ë” ê¸°ë°˜ ë¶„ë¥˜ ë° ì´ë™ (zip íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ ì§„í–‰)\n","if zip_file_extracted:\n","    for category in categories:\n","        # ì„ì‹œ í´ë” ë‚´ë¶€ ì–´ë””ë“  'cat' ë˜ëŠ” 'fish' í´ë”ê°€ ìˆëŠ”ì§€ ê²€ìƒ‰\n","        src_category_dir = find_category_path(temp_extract_path, category)\n","\n","        if src_category_dir is None:\n","            print(f\"âš ï¸ ê²½ê³ : ì••ì¶• íŒŒì¼ ë‚´ì—ì„œ '{category}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","            continue\n","        else:\n","            print(f\"ğŸ“‚ '{category}' í´ë” ë°œê²¬: {src_category_dir}\")\n","\n","        # í•´ë‹¹ í´ë” ë‚´ì˜ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n","        all_files = [f for f in os.listdir(src_category_dir)\n","                     if os.path.isfile(os.path.join(src_category_dir, f)) and not f.startswith('.')]\n","\n","        random.shuffle(all_files)\n","\n","        # ë¶„í•  ë¹„ìœ¨ (8:1:1)\n","        train_idx = int(len(all_files) * 0.8)\n","        val_idx = int(len(all_files) * 0.9)\n","\n","        # íŒŒì¼ ì´ë™\n","        for i, f in enumerate(all_files):\n","            src_path = os.path.join(src_category_dir, f)\n","            if i < train_idx:\n","                dst_path = os.path.join(base_path, 'train', category, f)\n","            elif i < val_idx:\n","                dst_path = os.path.join(base_path, 'val', category, f)\n","            else:\n","                dst_path = os.path.join(base_path, 'test', category, f)\n","            shutil.move(src_path, dst_path)\n","\n","# 6. ì„ì‹œ í´ë” ì‚­ì œ\n","if os.path.exists(temp_extract_path):\n","    shutil.rmtree(temp_extract_path)\n","print(\"\\nâœ¨ ëª¨ë“  ë¶„ë¥˜ ë° ì´ë™ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","\n","# 7. ê²°ê³¼ í™•ì¸\n","print(\"-\" * 30)\n","for s in splits:\n","    for c in categories:\n","        count = len(os.listdir(os.path.join(base_path, s, c)))\n","        print(f\"âœ… {s}/{c} í´ë” íŒŒì¼ ìˆ˜: {count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSQb3USmy2GD","executionInfo":{"status":"ok","timestamp":1769775244990,"user_tz":-540,"elapsed":3934,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"241972f3-016e-4655-c2af-b093335747b1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… 1. ì••ì¶• í•´ì œ ì™„ë£Œ\n","ğŸ“‚ 'cat' í´ë” ë°œê²¬: /content/temp_images/val/cat\n","ğŸ“‚ 'fish' í´ë” ë°œê²¬: /content/temp_images/val/fish\n","\n","âœ¨ ëª¨ë“  ë¶„ë¥˜ ë° ì´ë™ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n","------------------------------\n","âœ… train/cat í´ë” íŒŒì¼ ìˆ˜: 80\n","âœ… train/fish í´ë” íŒŒì¼ ìˆ˜: 44\n","âœ… val/cat í´ë” íŒŒì¼ ìˆ˜: 10\n","âœ… val/fish í´ë” íŒŒì¼ ìˆ˜: 5\n","âœ… test/cat í´ë” íŒŒì¼ ìˆ˜: 10\n","âœ… test/fish í´ë” íŒŒì¼ ìˆ˜: 6\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4LQYPx6aqQkg","executionInfo":{"status":"ok","timestamp":1769775273241,"user_tz":-540,"elapsed":540,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["def check_image(path):\n","    try:\n","        im = Image.open(path)\n","        return True\n","    except:\n","        return False\n","\n","img_transforms = transforms.Compose([\n","    transforms.Resize((64,64)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","    ])\n","train_data_path = \"./train/\"\n","train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n","val_data_path = \"./val/\"\n","val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)\n","\n","batch_size=64\n","train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SmG06TzWqQkh","executionInfo":{"status":"ok","timestamp":1769775275799,"user_tz":-540,"elapsed":20,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["cnnnet.to(device)\n","optimizer = optim.Adam(cnnnet.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8MKyoqjqQkj","executionInfo":{"status":"ok","timestamp":1769775322343,"user_tz":-540,"elapsed":45283,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"bd9fa14d-7d4d-4449-c806-7f50158dfb7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 2.87, Validation Loss: 0.54, accuracy = 0.80\n","Epoch: 2, Training Loss: 0.54, Validation Loss: 0.53, accuracy = 0.80\n","Epoch: 3, Training Loss: 0.52, Validation Loss: 0.51, accuracy = 0.80\n","Epoch: 4, Training Loss: 0.48, Validation Loss: 0.51, accuracy = 0.80\n","Epoch: 5, Training Loss: 0.47, Validation Loss: 0.63, accuracy = 0.80\n","Epoch: 6, Training Loss: 0.50, Validation Loss: 0.49, accuracy = 0.80\n","Epoch: 7, Training Loss: 0.46, Validation Loss: 0.55, accuracy = 0.80\n","Epoch: 8, Training Loss: 0.53, Validation Loss: 0.53, accuracy = 0.80\n","Epoch: 9, Training Loss: 0.51, Validation Loss: 0.50, accuracy = 0.80\n","Epoch: 10, Training Loss: 0.46, Validation Loss: 0.48, accuracy = 0.80\n"]}],"source":["train(cnnnet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=10, device=device)"]},{"cell_type":"markdown","metadata":{"id":"D-KrohEWqQkk"},"source":["## Downloading a pretrained network\n","\n","There are two ways of downloading pre-trained image models with PyTorch. Firstly, you can use the `torchvision.models` library, or you can use PyTorch Hub. The latter is preferred as of 2019, as this is a one-stop shop for all models and the new standard for distributing models with PyTorch."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTCzA4WaqQkl","executionInfo":{"status":"ok","timestamp":1769775332117,"user_tz":-540,"elapsed":4445,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"0fa14ff4-58c2-4688-c91d-67abc8e635ff"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233M/233M [00:02<00:00, 99.1MB/s]\n"]}],"source":["import torchvision.models as models\n","alexnet = models.alexnet(num_classes=1000, pretrained=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLXmiBU5qQkm","executionInfo":{"status":"ok","timestamp":1769775336927,"user_tz":-540,"elapsed":3036,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"931c42bf-3401-4238-e5b3-11c5ff7a7812"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]}],"source":["resnet50 = torch.hub.load('pytorch/vision', 'resnet50')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-u3Wr23qQkn","executionInfo":{"status":"ok","timestamp":1769775339481,"user_tz":-540,"elapsed":5,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"4268d25b-a2b1-47ba-bd40-5137e9876231"},"outputs":[{"output_type":"stream","name":"stdout","text":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")\n"]}],"source":["print(alexnet)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a48VlqsaqQkn","executionInfo":{"status":"ok","timestamp":1769775342002,"user_tz":-540,"elapsed":21,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"3bc401d3-72a1-4066-c498-3e9a3e07415b"},"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",")\n"]}],"source":["print(resnet50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvnIsg54qQkn"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}