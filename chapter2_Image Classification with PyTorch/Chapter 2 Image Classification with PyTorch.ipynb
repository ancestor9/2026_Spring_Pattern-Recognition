{"cells":[{"cell_type":"markdown","metadata":{"id":"TwJRECMMEsJ2"},"source":["# Chapter 2: Our First Model"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"TiPOZdvyEsJ6","executionInfo":{"status":"ok","timestamp":1769775100511,"user_tz":-540,"elapsed":7,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from PIL import Image, ImageFile\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES=True"]},{"cell_type":"markdown","metadata":{"id":"NNfLLQ8BEsJ7"},"source":["## Setting up DataLoaders\n","\n","We'll use the built-in dataset of `torchvision.datasets.ImageFolder` to quickly set up some dataloaders of downloaded cat and fish images.\n","\n","`check_image`  is a quick little function that is passed to the `is_valid_file` parameter in the ImageFolder and will do a sanity check to make sure PIL can actually open the file. We're going to use this in lieu of cleaning up the downloaded dataset.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_Gv6mODxEsJ7","executionInfo":{"status":"ok","timestamp":1769775100996,"user_tz":-540,"elapsed":13,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["def check_image(path):\n","    try:\n","        im = Image.open(path)\n","        return True\n","    except:\n","        return False"]},{"cell_type":"markdown","metadata":{"id":"tbxZ-bWREsJ7"},"source":["Set up the transforms for every image:\n","\n","* Resize to 64x64\n","* Convert to tensor\n","* Normalize using ImageNet mean & std\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"6VM-G-BIEsJ8","executionInfo":{"status":"ok","timestamp":1769775102260,"user_tz":-540,"elapsed":4,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"outputs":[],"source":["img_transforms = transforms.Compose([\n","    transforms.Resize((64,64)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                    std=[0.229, 0.224, 0.225] )\n","    ]\n",")"]},{"cell_type":"markdown","source":["### **ë°ì´í„°(Zip)ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ colabì— íŒŒì¼êµ¬ì¡°ì— ë”°ë¼ ì €ì¥í•˜ê¸°**"],"metadata":{"id":"IL2H7udAOuh0"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jFi2PQFQrYz","executionInfo":{"status":"ok","timestamp":1769775104809,"user_tz":-540,"elapsed":1286,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"1c44106e-3348-4772-9faa-ac5f9a8870f6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["zip_path ='/content/drive/MyDrive/Deep Learning/beginners-pytorch-deep-learning-master/chapter2_Image Classification with PyTorch/images.zip'"],"metadata":{"id":"HuCmf4-VQ6Vr","executionInfo":{"status":"ok","timestamp":1769775176243,"user_tz":-540,"elapsed":44,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import os\n","import zipfile\n","import shutil\n","import random\n","\n","# 1. í™˜ê²½ ì„¤ì •\n","base_path = '/content'\n","categories = ['cat', 'fish']\n","splits = ['train', 'val', 'test']\n","# zip_path = '/content/images.zip'  # zip íŒŒì¼ ì§ì ‘ upload í•˜ëŠ” ê²½ìš°\n","temp_extract_path = '/content/temp_images'\n","\n","# 2. ìµœì¢… ëª©ì ì§€ ë””ë ‰í† ë¦¬ ìƒì„±\n","for s in splits:\n","    for c in categories:\n","        os.makedirs(os.path.join(base_path, s, c), exist_ok=True)\n","\n","# 3. ì••ì¶• í•´ì œ\n","zip_file_extracted = False\n","if os.path.exists(zip_path):\n","    # ê¸°ì¡´ ì„ì‹œ í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒì„±\n","    if os.path.exists(temp_extract_path):\n","        shutil.rmtree(temp_extract_path)\n","\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(temp_extract_path)\n","    print(\"âœ… 1. ì••ì¶• í•´ì œ ì™„ë£Œ\")\n","    zip_file_extracted = True\n","else:\n","    print(f\"âŒ Error: {zip_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","# 4. ì‹¤ì œ cat, fish í´ë”ì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜\n","def find_category_path(root_path, target_category):\n","    for root, dirs, files in os.walk(root_path):\n","        if target_category in dirs:\n","            return os.path.join(root, target_category)\n","    return None\n","\n","# 5. í´ë” ê¸°ë°˜ ë¶„ë¥˜ ë° ì´ë™ (zip íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ ì§„í–‰)\n","if zip_file_extracted:\n","    for category in categories:\n","        # ì„ì‹œ í´ë” ë‚´ë¶€ ì–´ë””ë“  'cat' ë˜ëŠ” 'fish' í´ë”ê°€ ìˆëŠ”ì§€ ê²€ìƒ‰\n","        src_category_dir = find_category_path(temp_extract_path, category)\n","\n","        if src_category_dir is None:\n","            print(f\"âš ï¸ ê²½ê³ : ì••ì¶• íŒŒì¼ ë‚´ì—ì„œ '{category}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","            continue\n","        else:\n","            print(f\"ğŸ“‚ '{category}' í´ë” ë°œê²¬: {src_category_dir}\")\n","\n","        # í•´ë‹¹ í´ë” ë‚´ì˜ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n","        all_files = [f for f in os.listdir(src_category_dir)\n","                     if os.path.isfile(os.path.join(src_category_dir, f)) and not f.startswith('.')]\n","\n","        random.shuffle(all_files)\n","\n","        # ë¶„í•  ë¹„ìœ¨ (8:1:1)\n","        train_idx = int(len(all_files) * 0.8)\n","        val_idx = int(len(all_files) * 0.9)\n","\n","        # íŒŒì¼ ì´ë™\n","        for i, f in enumerate(all_files):\n","            src_path = os.path.join(src_category_dir, f)\n","            if i < train_idx:\n","                dst_path = os.path.join(base_path, 'train', category, f)\n","            elif i < val_idx:\n","                dst_path = os.path.join(base_path, 'val', category, f)\n","            else:\n","                dst_path = os.path.join(base_path, 'test', category, f)\n","            shutil.move(src_path, dst_path)\n","\n","# 6. ì„ì‹œ í´ë” ì‚­ì œ\n","if os.path.exists(temp_extract_path):\n","    shutil.rmtree(temp_extract_path)\n","print(\"\\nâœ¨ ëª¨ë“  ë¶„ë¥˜ ë° ì´ë™ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","\n","# 7. ê²°ê³¼ í™•ì¸\n","print(\"-\" * 30)\n","for s in splits:\n","    for c in categories:\n","        count = len(os.listdir(os.path.join(base_path, s, c)))\n","        print(f\"âœ… {s}/{c} í´ë” íŒŒì¼ ìˆ˜: {count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHiTWZb_O1Wv","executionInfo":{"status":"ok","timestamp":1769775185736,"user_tz":-540,"elapsed":4385,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"410dd02a-8baf-4588-fc5f-bf83d2510314"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… 1. ì••ì¶• í•´ì œ ì™„ë£Œ\n","ğŸ“‚ 'cat' í´ë” ë°œê²¬: /content/temp_images/val/cat\n","ğŸ“‚ 'fish' í´ë” ë°œê²¬: /content/temp_images/val/fish\n","\n","âœ¨ ëª¨ë“  ë¶„ë¥˜ ë° ì´ë™ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n","------------------------------\n","âœ… train/cat í´ë” íŒŒì¼ ìˆ˜: 80\n","âœ… train/fish í´ë” íŒŒì¼ ìˆ˜: 44\n","âœ… val/cat í´ë” íŒŒì¼ ìˆ˜: 10\n","âœ… val/fish í´ë” íŒŒì¼ ìˆ˜: 5\n","âœ… test/cat í´ë” íŒŒì¼ ìˆ˜: 10\n","âœ… test/fish í´ë” íŒŒì¼ ìˆ˜: 6\n"]}]},{"cell_type":"markdown","source":["### íŒŒì´í† ì¹˜(PyTorch)ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ë°ì´í„° ë¡œë”\n","- í˜„ì¬ ì €ì¥ë˜ì–´ ìˆëŠ” ì´ë¯¸ì§€íŒŒì¼ì„ ë¡œë”©"],"metadata":{"id":"yeQNBL7nSh09"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgJ9NuAlEsJ8"},"outputs":[],"source":["train_data_path = \"./train/\"\n","train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvbwkKanEsJ8"},"outputs":[],"source":["val_data_path = \"./val/\"\n","val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7RQq1VSEsJ8"},"outputs":[],"source":["test_data_path = \"./test/\"\n","test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_transforms, is_valid_file=check_image)"]},{"cell_type":"code","source":["val_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wfT4UMeTRmu","executionInfo":{"status":"ok","timestamp":1769734756567,"user_tz":-540,"elapsed":15,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"5d6bb496-fbfd-4ad3-9464-224080a240a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 18\n","    Root location: ./val/\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n","               ToTensor()\n","               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","           )"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["By default, PyTorchâ€™s data loaders are set to a batch_size of 1. You will almost certainly want to change that."],"metadata":{"id":"CdCGH9dvT5_M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikmSFOYDEsJ9"},"outputs":[],"source":["batch_size=64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsS_1cxqEsJ9"},"outputs":[],"source":["train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n","val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n","test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"]},{"cell_type":"code","source":["val_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUFd-a4oTZLH","executionInfo":{"status":"ok","timestamp":1769734756598,"user_tz":-540,"elapsed":6,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"b52ac8c3-5ff2-4eb6-d22e-b8a0181ad0d3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 18\n","    Root location: ./val/\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n","               ToTensor()\n","               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","           )"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"2GPBMHQoEsJ9"},"source":["## Our First Model, SimpleNet\n","\n","SimpleNet is a very simple combination of three Linear layers and ReLu activations between them. Note that as we don't do a `softmax()` in our `forward()`, we will need to make sure we do it in our training function during the validation phase."]},{"cell_type":"markdown","source":["  ğŸ—ï¸ SimpleNet ì½”ë“œ ìƒì„¸ ë¶„ì„\n","\n","    1. í´ë˜ìŠ¤ ì •ì˜ ë° ì´ˆê¸°í™” (__init__)\n","    ì‹ ê²½ë§ì˜ **ì¬ë£Œ(ë¶€í’ˆ)**ë¥¼ ì¤€ë¹„í•˜ëŠ” ë‹¨ê³„\n","\n","        class SimpleNet(nn.Module): # nn.Moduleì„ ìƒì†ë°›ì•„ íŒŒì´í† ì¹˜ ëª¨ë¸ì„ì„ ì„ ì–¸\n","            def __init__(self):\n","                super(SimpleNet, self).__init__() # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”\n","                # ì²« ë²ˆì§¸ ì¸µ: ì…ë ¥ 12288ê°œ -> ì¶œë ¥ 84ê°œ\n","                # 12288ì€ ì´ë¯¸ì§€ í¬ê¸°ê°€ 64x64x3(RGB)ì¼ ë•Œ (64*64*3 = 12288)\n","                self.fc1 = nn.Linear(12288, 84)\n","\n","                # ë‘ ë²ˆì§¸ ì¸µ: ì…ë ¥ 84ê°œ -> ì¶œë ¥ 50ê°œ\n","                self.fc2 = nn.Linear(84, 50)\n","\n","                # ì„¸ ë²ˆì§¸ ì¸µ (ì¶œë ¥ì¸µ): ì…ë ¥ 50ê°œ -> ì¶œë ¥ 2ê°œ\n","                # ë¶„ë¥˜í•  ëŒ€ìƒì´ 'cat', 'fish' 2ê°œì´ë¯€ë¡œ ìµœì¢… ì¶œë ¥ì€ 2ê°œ\n","                self.fc3 = nn.Linear(50, 2)\n","\n","    2. ìˆœì „íŒŒ ì—°ì‚° (forward)\n","    ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì–´ë–¤ ìˆœì„œë¡œ ê³„ì‚°ë ì§€ ì„¤ê³„ë„ë¥¼ ê·¸ë¦¬ëŠ” ë‹¨ê³„\n","\n","            def forward(self, x):\n","                # 1. í¼ì¹˜ê¸° (Flatten)\n","                # x.view(-1, 12288): 3ì°¨ì› ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ\n","                # batch_sizeê°€ 64ë¼ë©´ xì˜ í¬ê¸°ëŠ” [64, 12288]\n","                x = x.view(-1, 12288)\n","\n","                # 2. ì²« ë²ˆì§¸ ì¸µ í†µê³¼ + ReLU í™œì„±í™” í•¨ìˆ˜\n","                # ReLUëŠ” 0ë³´ë‹¤ ì‘ì€ ê°’ì€ 0ìœ¼ë¡œ, í° ê°’ì€ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œì¼œ í•™ìŠµ\n","                x = F.relu(self.fc1(x))\n","                \n","                # 3. ë‘ ë²ˆì§¸ ì¸µ í†µê³¼ + ReLU\n","                x = F.relu(self.fc2(x))\n","                \n","                # 4. ë§ˆì§€ë§‰ ì¶œë ¥ì¸µ í†µê³¼\n","                # ê²°ê³¼ê°’ì€ [64, 2] í¬ê¸°ì˜ í…ì„œ (ê° ì´ë¯¸ì§€ë‹¹ cat/fish í™•ë¥  ì ìˆ˜)\n","                x = self.fc3(x)\n","                return x"],"metadata":{"id":"A2XC3ltLV9pm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"glt314xSEsJ9"},"outputs":[],"source":["class SimpleNet(nn.Module):\n","\n","    def __init__(self):\n","        super(SimpleNet, self).__init__()\n","        self.fc1 = nn.Linear(12288, 84) #\n","        self.fc2 = nn.Linear(84, 50)\n","        self.fc3 = nn.Linear(50,2)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 12288)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw0HJ1xUEsJ9"},"outputs":[],"source":["simplenet = SimpleNet()"]},{"cell_type":"markdown","metadata":{"id":"7OgTK0LNEsJ-"},"source":["## Create an optimizer\n","\n","Here, we're just using Adam as our optimizer with a learning rate of 0.001."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aovv_zpyEsJ-"},"outputs":[],"source":["optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"T2a4GvpMEsJ-"},"source":["## Copy the model to GPU\n","\n","Copy the model to the GPU if available."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wc-ZAkrEsJ-","executionInfo":{"status":"ok","timestamp":1769734756643,"user_tz":-540,"elapsed":17,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"f4cbd839-7ceb-422b-80a6-f8e356b9cb82"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SimpleNet(\n","  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n","  (fc2): Linear(in_features=84, out_features=50, bias=True)\n","  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":71}],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","simplenet.to(device)"]},{"cell_type":"markdown","metadata":{"id":"OaevpNfKEsJ-"},"source":["## Training\n","\n","Trains the model, copying batches to the GPU if required, calculating losses, optimizing the network and perform validation for each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JO73twWSEsJ-"},"outputs":[],"source":["def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n","    for epoch in range(1, epochs+1):\n","        training_loss = 0.0\n","        valid_loss = 0.0\n","        model.train()\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            inputs, targets = batch\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","            output = model(inputs)\n","            loss = loss_fn(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","            training_loss += loss.data.item() * inputs.size(0)\n","        training_loss /= len(train_loader.dataset)\n","\n","        model.eval()\n","        num_correct = 0\n","        num_examples = 0\n","        for batch in val_loader:\n","            inputs, targets = batch\n","            inputs = inputs.to(device)\n","            output = model(inputs)\n","            targets = targets.to(device)\n","            loss = loss_fn(output,targets)\n","            valid_loss += loss.data.item() * inputs.size(0)\n","            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n","            num_correct += torch.sum(correct).item()\n","            num_examples += correct.shape[0]\n","        valid_loss /= len(val_loader.dataset)\n","\n","        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n","        valid_loss, num_correct / num_examples))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ktD6NpsHEsJ-","executionInfo":{"status":"ok","timestamp":1769734759594,"user_tz":-540,"elapsed":2948,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"20900e26-0a82-4b22-c707-ee9be2444173"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 0.71, Validation Loss: 0.73, accuracy = 0.61\n","Epoch: 2, Training Loss: 0.39, Validation Loss: 0.20, accuracy = 0.89\n","Epoch: 3, Training Loss: 0.09, Validation Loss: 0.14, accuracy = 0.94\n","Epoch: 4, Training Loss: 0.06, Validation Loss: 0.08, accuracy = 1.00\n","Epoch: 5, Training Loss: 0.03, Validation Loss: 0.04, accuracy = 1.00\n"]}],"source":["train(simplenet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=5, device=device)"]},{"cell_type":"markdown","metadata":{"id":"tf9FyqFpEsJ-"},"source":["## Making predictions\n","\n","Labels are in alphanumeric order, so `cat` will be 0, `fish` will be 1. We'll need to transform the image and also make sure that the resulting tensor is copied to the appropriate device before applying our model to it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jNQUUBEaEsJ-","executionInfo":{"status":"ok","timestamp":1769734759613,"user_tz":-540,"elapsed":17,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"34f0ded0-97b9-49df-b3f8-ded390a63149"},"outputs":[{"output_type":"stream","name":"stdout","text":["fish\n"]}],"source":["labels = ['cat','fish']\n","\n","img = Image.open(\"./val/fish/DSCN0326.jpg\")\n","img = img_transforms(img).to(device)\n","img = torch.unsqueeze(img, 0)\n","\n","simplenet.eval()\n","prediction = F.softmax(simplenet(img), dim=1)\n","prediction = prediction.argmax()\n","print(labels[prediction])"]},{"cell_type":"markdown","metadata":{"id":"b4p3oVy5EsJ-"},"source":["## Saving Models\n","\n","We can either save the entire model using `save` or just the parameters using `state_dict`. Using the latter is normally preferable, as it allows you to reuse parameters even if the model's structure changes (or apply parameters from one model to another)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_3NWmqWEsJ_"},"outputs":[],"source":["torch.save(simplenet, \"/tmp/simplenet\")\n","simplenet = torch.load(\"/tmp/simplenet\", weights_only=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAsIzpslEsJ_","executionInfo":{"status":"ok","timestamp":1769734759694,"user_tz":-540,"elapsed":47,"user":{"displayName":"sanggoo cho","userId":"05013997244878666178"}},"outputId":"16661c66-5bb7-46fe-dda0-4abd3f36ea77"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":76}],"source":["torch.save(simplenet.state_dict(), \"/tmp/simplenet\")\n","simplenet = SimpleNet()\n","simplenet_state_dict = torch.load(\"/tmp/simplenet\")\n","simplenet.load_state_dict(simplenet_state_dict)"]},{"cell_type":"code","source":[],"metadata":{"id":"_WwW7nsoaI84"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}